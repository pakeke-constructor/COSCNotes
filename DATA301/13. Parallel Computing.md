# 13. Parallel Computing

MapReduce architecture gives us parallelization for 'free', but comes with trade-offs and overhead that may be too large for certain applications - if the algorithm is complex but requires little data transfer and movement (e.g. scientific simulations).

In the 90s, if you want your program to run twice as fast, you can either do a lot of work optimizing the program or... wait 18 months and buy a new computer. Back then, scaling was pretty simple: the clock speed would increase, so no work was needed to take advantage of the new hardware.

There is no more free lunch any more - hardware does not improve (quickly) in a way that makes your program run faster automatically. However, transistor density is still and increasing and there are more parallel units on a chip; we must rewrite our programs to take advantage of this hardware.

## Types of Parallelism

### Data Dependence Graph

Break a problem down into its components, and create a DAG out of it. Vertices are tasks and edges are dependencies.

Critical path: if any task in the critical path takes longer, it slows down the entire system

Independence: if a task is independent of another, it can run simultaneously with that task

### Data Parallelism

Independent tasks that apply the same operations to different elements of a data set: operations can be performed concurrently e.g. `a = [b[i] + c[i] for i in range(100)]`.

### Functional Parallelism

Independent tasks apply *different* operations to different data elements e.g.

```
     a = 10;       b = 4;

c = 2 * a + b;    d = a ** 2

        e = c + d;
```

Processor will execute instructions out-of-order to execute instructions in parallel.

### Pipelining

Divide a process into stages to produce several items simultaneously - an assembly line.

If a task is highly sequential, although each stage can only be done by a single process, all stages can be done simultaneously with each stage working on a separate item.
