# 18. GPU-Based Programming Concepts

## Shared Memory Programming

Works best when the access latency is small - hence, with small memory spaces.

A common workload which works on a small amount of memory and requires many processes is in graphics processing.

CPU:

```
----------------------
|         | ALU | ALU |
| Control |     |     |
          -------------
|         | ALU | ALU |
|         |     |     |
-----------------------
| Cache               |
|                     |
|                     |
|                     |
-----------------------
-----------------------
| DRAM                |
-----------------------
```

NB: cache and DRAM more continuous now.

- The closer the memory is to the core, the lower the latency.
- ALUs now very large and complex as it must support a large number of operations

GPU:
```
-----------------------
|C/C| ALU | ALU | ALU |
-----------------------
|C/C| ALU | ALU | ALU |
-----------------------
|C/C| ALU | ALU | ALU |
-----------------------
|C/C| ALU | ALU | ALU |
-----------------------
|C/C| ALU | ALU | ALU |
-----------------------
-----------------------
| DRAM                |
-----------------------
```

Many simple ALUs that share tiny control and cache structures.

More control over memory; can choose if a variable local, shared or global (three levels from cache). Also off-memory (CPU memory).

### CUDA Model

**Grid-based decomposition**: the grid is the entire program, which is split into blocks.

**Blocks** are independent subtasks that can be run in any order. They can share memory with each other.

**Threads** in each block run the same instructions simultaneously but accessing different bit of memory. Hence, only one set of control logic is required for each block.

Thread blocks must be able to run **independently**; execute in any order, in parallel etc.

For thread safety, cannot read/write to the same memory address at the same time.

### [Numba](https://nyu-cds.github.io/python-numba/)

JIT Python library that can compile Python into machine code.

```python
from numba import jit

# NB: `%timeit` returns the amount of taken to run a specific line of code in Jupyter

@jit
def bubblesort_jit(arr):
  N = len(arr)

  for end in range(N, 1, -1):
    for i in range(end - 1):
      cur = arr[i]

      if cur > arr[i + 1]:
        tmp = arr[i]
        arr[i] = arr[i + 1]
        arr[i + 1] = tmp
```

On the GPU:

```python
from numba import cuda

@cuda.jit
def my_kernel(io_array):
  # io_array stored in shared memory
  pos = cuda.grid(1) # ID of the thread; like `rank` in MPI
  # Kinda like my_block_number * threads_per_block + my_thread_number
  # Can use cuda.gridsize(1) to get number of blocks/grids
  if pos < io_array.size:
    io_array[pos] = pos

data = numpy.ones(257)
threads_per_block = 256 # 256 threads in each block. Max ~1024 on most Nvidia GPUs, or less if it is a memory intensive program
blocks_per_grid = 1 # and just one block

# data copied from CPU memory to GPU shared memory
my_kernel[blocks_per_grid, threads_per_block](data)

print(data)
# Should print `0, 1, 2, ..., 254, 255, 1`
```

There are no return statements in CUDA; instead, pass in an array (e.g. `cuda.to_device(np.zeros(shape = 1, dtype=np.int32))`).

#### Atomic Operations

Incrementing a counter is not an atomic operation: it loads, adds then stores. Hence, if it is written after it has been read, the result will be overwritten. Hence, **atomic operations** are required to do this in a single step.

```python
cuda.atomic.add(arr, index, increment_by)

cuda.atomic.compare_and_swap(arr, old, new) # if arr[0] == old, set it to new
```

Matrix multiplication: sum-multiply row of matrix A with column of matrix B to get one result.

```python
@cuda.jit
def matrix_mul(A, B, C):
  row, col = cuda.grid(2) # Interpret ID as 2D position

  if row < C.shape[0] and col < C.shape[1]:
    # If row less than number of rows in C and col less than number of columns in C 
    temp = 0
    for k in range(A.shape[1]):
      tmp += A[row, k] * B[k, col]

    C[row, col] = tmp
```

The memory access pattern for this is not great for one of A and B: memory is accessed contiguously so if the memory is row-oriented, reading a row of A will read from a single contiguous area of memory but reading a column of B will require a read for each element.

Numpy arrays are made in the CPU and then sent to the GPU. However, the array can be made directly on the GPU:

```python
# Where `shape` is a tuple of dimension sizes e.g. (4, 6) for 4x6 2D array
# and `dtype` is a type such as `float32`
global_arr = cuda.device_array(shape)
shared_arr = cuda.shared.array(shape, dtype)
```

##### Locks

An alternative to atomic operations (that is available on CPUs); in a critical section of code, you can tell the hardware to not let anyone else read/write the value; **mutual exclusion**. This blocks other processes from running and so is slow.

Spin lock:

```c
volatile int lock = 0;

while(true) {
  if (lock == 0) {
    lock = 1;
    break;
  }
}
```

Polling is inefficient.

Atomic lock: make sure no one gets between load and store using a common primitive (**compare-and-swap**). Write only if the value in memory matches the expected value.

```c
temp = *addr;

if (temp == old) {
  *addr = new;
} else {
  old = temp;
  // and try, try again 
}
```

This is called `CMPXCHG` (compare and exchange) in x86.
