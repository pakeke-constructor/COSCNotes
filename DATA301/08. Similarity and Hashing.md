# DATA301 Lecture 08 Similarity and Hashing

## Similarity

TODO MERGE PREV LECTURE

Finding near-neighbours in a high-dimensional space.

### Jaccard Similarity

If dimensions are boolean values (e.g. if some specific keyword present or not):

$$
\textrm{similarity}(C_1. C_2) = \frac{| C_1 \cap C_2 |}{| \C_1 \cup C_2 |}
$$

Jaccard bag similarity:

- Don't remove duplicates; if number appears in intersection multiple times, count it multiple times
  - e.g. count the number of pairs
- Size of union is sum of sizes of the sets
- Max similarity is 0.5

### Cosine Distance

Angle whose cosine is the normalized dot product of the vectors:

$$
d(p_1, p_2) = \theta = \arccos{\frac{p_1 \cdot p_2}{|p_1| \cdot |p_2|})}
$$

Converting to frequency set: convert from array to table with value and number of times the value appears in the array.

### Other Distance Measures

- $L_1$ norm: Manhattan difference; sum of differences in each dimension
- TODO

## Hashing

Given a large number ($N > 1,000,000$) of documents, find 'near duplicate' pairs.

Hashing has previously been useful for finding exact matches; for this problem, we need a *locality-sensitive* hash that puts similar documents into the same bucket.

### Shingling

Converting documents to sets (e.g. set of words).

*k*-shingle or *k*-gram is a sequence of *k* tokens that appears in a document:

- Documents can be characters or words (usually characters)

e.g. `abcab` 2-gram: becomes `ab`, `bc`, `ca` (and `ab` if duplicates are being included).

When words are misspelt, only a few characters are wrong/out of order and hence, most of the shingles are likely to be the same as the correctly-spelt word.

TODO compression

Caveat: *k* value must be picked appropriately:

- $k = 5$ okay for short documents
- $k = 10$ better for long document

Sampling may be required for larger datasets.

TODO

#####

- Rows: shingles
- Columns: sets (documents)
- `matrix[e][s]` is `1` if shingle `shingles[e]` appears in document `s`
- Column similarity is the Jaccard similarity of the corresponding sets

However, this approach generates a sparse matrix and requires $O(n^2)$ space.

TODO

Need to find a hash function $h$ such that

If $sim(C_1, C_2)$ is high, there is a high probability that $h(C_1) = h(C_2)$ and vice versa if they are dissimilar.

#### Min-Hashing

Pseudo-randomly generated permutation (ordering) of shingles.

Go to first document, first permutation. If 

TODO

singature matrix $M$

Proved that:

$$
P(h_\pi(C_1) = h_\pi(C_2)) \approx sim(C_1, C_2)
$$

Similarity of two signatures: fraction of hash functions in which they agree.

Can use multiple permutations to increase probability.

### Locality-Sensitive Hashing

Find documents with Jaccard similarity of at least $s$.

Break hash key into pieces and put them into different buckets. That way, even if a few values are different, TODO

