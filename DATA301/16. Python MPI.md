# 16. Python MPI

A set of functions that extend the language towards a paradigm where each process has its own memory space and can share and receive messages.

## Message Passing

To send a message to another process, the process puts the data into a buffer and calls a send function. The receiving process must have acknowledged that it wants to receive it; send and receive function calls are always paired.

MPI is responsible for the internals of actually sending the data.

The programs are initially two completely separate programs; MPI must open a communication channel that can communicate between programs.

```python
from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size() # number of processes connected to the world
rank = comm.Get_rank() # process identifier. Starts at 0

comm.Send(var_to_send     , dest=${process_to_send_to})
comm.Recv(var_to_overwrite, dest=${process_to_receive_to})
```

The variable you send/receive must map to a byte buffer (e.g. numpy arrays). The receive buffer can be larger than the send buffer but not the reverse (`MPI_ERR_TRUNCATE` in Python - if it's in C it will probably just write past the end of the structure).

The send and receive functions also have a `tag` argument: a value to identify the type of message being sent.

There is also a special `MPI.ANY_SOURCE` to receive messages from any process. To figure out what rank the message came from:

```python
message = numpy.zeros(1)
status = MPI.Status()
comm.Recv(message, source=MPI.ANY_SOURCE, status=status)
print(status.source)
```

You can also use `comm.Probe()` to get the `status` and `tag` values before actually receiving the value; this can be useful if the rank determines the index of the array to write to.


To run the program:

```bash
mpiexec -n ${num_processes} python file.py

# For Colab:
mpiexec --allow-run-as-root -n ${num_processes} python file.py # --allow-run-as-root flag required
```

In the Jupyter code block, add the magic command `%%writefile filename.py` to the top of the file; when the code block is `run`, it saves the contents to the file.

### Non-Blocking Communication

Allows you to do a small amount o communication while you wait for the message to be sent or response to be received.

```python
req = comm.Isend(data, dest=RANK)
# or
req = comm.Irecv(data, source=RANK)
req.Wait()
```

### Collective Operations

A way to send and collect a lot of data.

Broadcasting: sending message to all processes; `Comm.Bcast`

Scatter: sending chunks of an array to each process; `Comm.Scatter`

Gather: taking chunks of arrays from many processes and combining them; `Comm.Gather`

## Computational Science

- Experiment
- Theory
- Simulate ('theoretical experiments')

Simulating requires much more computation than communication.

### Parallelism

Often, a large group of operations can be done concurrently without memory conflicts.

Sometimes, each cell update affects cells with neighbouring faces. Hence, cells that do share faces can be updated simultaneously. Communication is required to reduce the forces from neighbouring cells.

e.g. brick wall: if each person has their own horizontal slice of the wall, at the boundaries of their section, they must ensure their neighbours have completed their own layer so that there are no gaps.

Multiple-scale applications: sometimes, different sections of the simulation take longer - some cells may more or less updates than neighbouring cells, or there may be more 'interesting' areas that need to be simulated in higher resolution.

#### N-body problem

Finding the position and velocities of a collection of interacting particles.
