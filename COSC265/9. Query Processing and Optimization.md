# 9. Query Processing and Optimization

$\pi_{lname, fname}(\sigma_{snumber = star \land movie = mnumber \land title = 'Gone with the wind'}(star \times stars \times movie))$

If this is done naively, it will get the Cartesian product of the three tables (if each has 100 rows, then this results in 1,000,000 rows) then gets the names of stars that performed in a particular movie - probably something like 10 rows. Needless to say, this produces a lot of garbage rows and is very inefficient. In a real database query optimization is done.

## Query Processing

- Scanning, parsing and validating -> immediate form of query
- Query optimizer -> execution plan
- Query code generator -> code to execute the query
- Runtime database processor -> result

Code can be executed directly or stored and executer later (compiled).

## Translating SQL Queries into Relational Algebra

**Query block**: the basic unit that can be translated into algebraic operators and optimized. This is a single `SELECT ... FROM ... WHERE [... GROUP BY ... HAVING]` expression. Nested queries will have multiple query blocks.

The query `SELECT name FROM employee WHERE salary > (SELECT MAX(salary) FROM employee WHERE dept = 5)` can be split into:

- `SELECT name FROM employee WHERE salary > C`
  - $\pi_{name}(\sigma_{salary > c}(employee))$
- `SELECT MAX(salary) FROM employee WHERE dept = 5`
  - $\mathcal{F}_{MAX salary}(\sigma_{DNO=5}(employee))$

For the second query, an unoptimized process could be:

- Retrieve all blocks from employee, filter out those with the wrong department, and sort them - unlikely to be indexed or be sorted a key salary
  - If there a lot of rows, sorting may need to be done using external sorting
- Find the top row that has the maximum salary

### Side Note: Sort-Merge

- Read *x* rows, sort them (quicksort etc.) and write to disk
- Repeat *n* times
- Read the first *x/(n+1)* rows from each subfile and perform an *n*-way merge. Store the result in a buffer; when the buffer size reaches *x*, write it to disk

The process can be done recursively.

### Algorithms for `WHERE`

Simple selection has only as single condition; complex selections have multiple conditions.

**Selectivity**: ratio of the number of tuples that satisfy the condition to the total number of tuples

For simple selection:

- Linear search: no index, unstructured data
- Binary search: no index, sorted data
- Primary index/hash key to retrieve a single record
- Primary index to retrieve multiple records (inequalities)
- Clustering index to retrieve multiple records (equality)
- Secondary index

For complex selection:

- Conjunctive selection using an index
  - Use the index for a simple condition, then check if the retrieved tuple satisfies the other conditions
- Conjunctive selection using a composite index (index exists for two or more attributes involved in the conditions)
- Conjunctive selection using the intersections of record pointers
  -Secondary indexes available for some of the attributes involved in the conditions, providing pointers to records (not blocks)
  - Get the record pointers from each index, and find the intersection between them
  - Then, check if  the retrieved tuples satisfy the other conditions
- Disjunctive conditions (union)
  - If there are no indexes, use brute force

### Algorithms for `JOIN`

Cross join/Cartesian product is time consuming. Equi-join and natural joins are much more efficient. Join performance is affected by:

- Available buffer space
- Join selection factor - fraction of records in one file that will be joined with records in the other file
- Choice of inner/outer relation

If an index/hash key exists for one of the join attributes, a **single-loop join** can be performed: read all rows **from the table without the index** and use the index to directly retrieve matching records. If this is not available, a **nested-loop join** is required (brute force).

### Algorithms for `SELECT`

If the attribute list contains candidate keys, all that is required is removing unwanted attributes.

Otherwise, duplicates must be removed. This can be done through either sorting or hashing.

### Algorithms for Set Operations

Sort both relations on the same attributes, then scan and merge sorted files concurrently, deciding on if the tuple(s) should be kept by checking their equality.

### Algorithms for Aggregate Functions

Min, max:

- If an index exists, this can be used. If want to find the maximum and an ascending index exists, the right-most pointer in each index node can be followed
- Otherwise, each row must be read

Sum, count, average:

- For a dense index, the computation can be done on the values in the index
- If an index does not exist or it is a non-dense index, each row must be read

Group by:

- Sorting or hashing is used to partition the files into appropriate groups
- The aggregate function can be done on tuples in each group

## Query Optimization

Some optimizations that can be done on the first example:

- The condition, `title = 'Gone with the wind` involves only a single table, so the select operation can be done before the Cartesian product. If eah table has 100 rows, then the resulting table will only have 100,000 rows
- Only the movie ID is needed from the movie table so a project operation can be done to only keep the movie ID before executing the Cartesian product
- Use a join instead of a Cartesian product and select clause

$\pi_{lname, fname}(star \bowtie_{snumber = star} stars \bowtie_{movie = mnumber} \pi_{movie\_id}(\sigma_{title = 'Gone with the wind'}(movie)))$

### Heuristics

### Cost-Based

Catalogs store metadata about tables - e.g. number of rows in each table stored in the catalog instead of requiring a `SELECT COUNT(*) FROM table` statement.

### Semantics

## Query Optimization in Oracle
