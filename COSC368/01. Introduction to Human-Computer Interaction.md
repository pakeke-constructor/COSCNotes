# 01. Introduction to Human-Computer Interaction

Andy Cockburn: Room 313, working Thursdays and Fridays

Tutors: `team368@cosc.canterbury.ac.nz`

Course breakdown:

- Labs: 9%, 1% per lab
- Usability analysis and storyboard
  - 25%, 5pm 22 September
- Design specification and rationale
  - 15%, 5pm 20 October
- Exam: 51%

Goals:

- Understand key human factors influencing HCI
- Know and apply guidelines, models, methods that aid in interface design

HCI: discipline concerned with the design, evaluation and implementation of interactive computing systems for human use.

There should be a cycle of evaluating, designing and implementation.

## Usability

Three key pillars:

- Learnability: rapid attainment of some level of performance
  - Can be modelled as the inverse of time spent on the interface
- Efficiency: can get a lot of work done per unit time
- Subjective satisfaction: how much you enjoy using it

Two minor pillars:

- Errors: should be few errors in an efficient interface.
- Memorability: should be memorable if the interface is learnable.

Trade-offs: efficiency and learnability (inverse of time spent) are often at odds with each other. The performance/efficiency ceiling is often lower for more learnable interfaces.

### Preliminary Factors

- Safety considerations
- Need for throughput (efficiency)
- Frequency of use
- Physical space, lighting, noise, pollution
- Social context
- Cognitive factors: age, fatigue, stress, focus

Usability is like oxygen: you only notice it when it's absent. See: doors with handles that you need to push.

### Managing Complexity

The job of HCI is to manage complexity: designing an object to be simple and clear; **the relentless pursuit of simplicity**.


```
Interface
Complexity
    ^
    |                                              ____
    |                                         ____/
    | Poorly designed                    ____/
    | UIs; complexity               ____/
    | amplified                ____/
    |                     ____/     Well designed UIs
    |                ____/
    |           ____/
    |      ____/
    | ____/
    |/
    +--------------------------------------------------> Domain
     Door      Word        CAD          Nuclear          Complexity
             Processor                power plant
```

### Models

Models are **simplifications of reality** that (should) help with the understanding of a complex artifact.

#### Don Norman's Model of Interaction

From 'The Psychology/Design of Everyday Things', 1988.

This helps understand the designer's role in creating a system that is used by a thinking person.

```
               constructs
   Designer/ -------------> System/system image
designer model                ^
                    Provides  | Provides input based on
                    feedback/ | their prediction of how
                     output   |  to achieve their goal 
                              v
                             User/
                          user model
```

The designer tries to construct a system that they have **not fully defined**. The *designer's model* is their *conception of interaction*; often incomplete, fuzzy or compromised in the actual implementation.

System image: the way the system *appears* to the user; this does not necessarily reflect the truth of the system.

<!-- System image: sum of all information (previous experience, advertisements, manuals etc.) available about the system. The information may be wrong/incomplete. -->

The *user model* begins very weak, coming from familiarity with the real world or other similar systems. They will use this experience to try and interact with the user system, building their model based on *feedback* from the system.

Ideally, there should be conformance between the system image and user model.

There is no direct communication between the designer and user; the designer can only communicate with the user through the system.

##### Execute-Evaluate Cycle

Execute:

- Goal -> Intention -> Actions -> Execution
  - The user has a *goal* and know the outcome they want
  - They form an *intention* translate this to the language of the user interface; an *action*
  - 'Gulf of Execution': problems executing intentions/actions
- Evaluate
  - Perceive -> Interpret -> Evaluate
    - *Perceive* the response/feedback by the system to their actions
    - *Evaluate*; determine the effect of their action. Did it meet their goal?
    - 'Gulf of Evaluation': problems assessing state, determining effect etc.

#### UISO Interaction and Framework

Emphasizes translation during interaction

Articulation: user's task language to input language
Performance: callbacks etc.
Presentation: show new state
Observation: interpretation

User has some low level task (e.g. saving a file); they need to translate their intention to an input language; this is one of the most difficult parts of user interface design.

```
              --> Output ---
Presentation /              \ Observation
            /                \
           /                  v
    System                      User
    (Core)                      (Task)
           ^                  /
Performance \                / Articulation
             \--- Input <---/
```

### Mappings

Good mappings between user and input/output increase usability.

#### Affordances

Objects afford particular actions to users:

- Door handles afford pulling
- Dials afford turning
- Buttons afford pushing
- Bush shelters
  - Glass affords smashing
  - Plywood affords graffiti

Poor affordances encourages incorrect actions, but strong affordances may stifle efficiency.

#### Over-/Under-determined Dialogues

- Well-determined: natural translation from task to input language
- Under-determined: user knows what they want to do but not how to do it
  - e.g. command line
- Over-determined: user forced through unnecessary or unnatural steps
  - e.g. 'Click OK to proceed', lengthy wizards
  - User turns into a robot; no freedom in what to do

Beginner user interface designers tend to think about the interface in terms of system requirements: the system needs x, y, z information so lets ask the user about these things up-front. These over-determined dialogues lead to horrible design.

#### Direct Manipulation

- Visibility of objects
- Direct, rapid, incremental and **reversible** actions:
- Reversibility allows users **no-risk exploration of the user interface**
- Rapid feedback
- Syntactic correctness
  - Disable illegal actions (e.g. greying buttons out when action not available)
    - Tooltips can help with the problem of not knowing why the action is not available
- Replace language with action
  - Language needs to be learned and remembered (e.g. command lines)
  - Actions; see and point

Advantages:

- Easy to learn
- Low memory requirements
- Easy to undo
- Immediate feedback to user actions
- Users can use spatial cues

Disadvantages:

- Consumes more screen real estate
- High graphical system requirements
- May trap users in 'beginner mode'

## The Human

- Input: vision, hearing, haptics
- Output: pointing, steering, speech, typing etc.
- Processing: visual search (slow), decision times (fast), learning
- Memory
- Phenomena and collaboration
- Error (predictably irrational behaviour)

### Fun Example

A trivial task that many humans will get wrong.

Count the number of occurrences of the letter 'f' given a set of words:

> Finished files are the results of years of scientific study combined with the experience of many years

Three phonetics Fs: 'finished', 'files', 'scientific', are easily found.

But three non-phonetic Fs in 'of' are often forgotten.

### [Click](http://blogoscoped.com/click2/)

Even a blank graphic has affordances on where people usually click: on or near the center, or along the diagonals or corners.

### Human Factors

Psychological and physiological abilities hae implications for design:

- Perception: how we perceive things
- Cognitive: how we process information
- Motor: how we perform actions
- Social: how we interact with others

### The Human Information Processor

Card, Moran, Newell 1983.

<!-- GOMS:

- Goal
- Operators: ways to go about executing the task
- Methods: alternate methods to execute
- Selectors: selecting methods

Sub-divide into low-level (keystrokes) tasks.

KLM:

-  -->

```
               Eyes/Ears
                   │
                   ▼
    ┌──── Perceptual Processor ────┐
    │                              │
    ▼                              ▼
Visual Image ──────┬─────── Auditory Image
 Storage           │           Storage
                   │
                   ▼
      ┌─────Working Memory ◄─────────┐
      ▼                  ▲           ▼
    Motor                │        Long-Term
  Processor              │         Memory
      │                  │           ▲
      |                  |           |
      ▼                  │           ▼
  Movement               └──────► Cognitive
  Response                        Processor 
```

### Human Input

#### Vision

Cells:

- Rods: low light, monochrome, 100 million rods across the retina
- Cones: color, 6 million rods in fovea
  - S/M/L for short/medium/long approx blue/green/reddish-yellow sensitivity

Areas:

- Retina: ~120 degree range, sensitive to movement
  - ~210 degrees with both eyes
  - Notifications popping up in corners etc. will distract user
- Fovea: detailed vision, area of ~2 degrees

1 degree = 60 arcminutes, 1 arcminute = 60 arcseconds

Visual Acuity:

- Point acuity; maximum angle between two dots before they become indistinct: 1 arcminute
- Grating acuity; maximum angle between alternating bars before they become indistinct: 1-2 arcminutes
- Letter acuity: 5 minutes of arc
- Vernier acuity: given two parallel lines, minimum angle of separation in normal axis (e.g. `---___`) before they are perceived as a continuous line (colinear): 10 arcseconds

Eye movement:

- Fixations: visual processing occurs only when the eye is stationary
- Saccades: rapid eye movements; about 900 degrees per second
  - Blind while saccades are in progress
- Eye movement as input; difficult as people don't have much control over where they are looking (e.g. accidentally looking at 'delete all my files' button)
- Smooth-pursuit: ability to tracking moving objects (up to 100 degrees per second)
  - **Cannot be induced voluntarily** - can't imagine a moving dot and track it
  - Relevant in scrolling

Size/depth cues:

- Familiarity
- Linear perspective; straight lines getting closer together
- Horizontal distance
- Size constancy: if object gets bigger/smaller, it's probably the object moving closer/further away, not the object changing size
- Texture gradient: texture getting bigger/smaller
- Occlusion: occluded items further away
- Depth of focus: blurrier the further you go away
- Aerial perspective: blurrier and bluer from atmospheric haze
- Shadows/shading
- Stereoscopy (best within 1m, ineffective beyond 10m)

Muller-Lyer illusion:

```
 <->
>---<
```

Bottom one looks further away and is subtending the same angle, so brain perceives it as bigger.

3D, depth-based UIs:

- The world is 3D so all interaction should be 3D, right?
- Occlusion, far-away things being smaller, navigation/orientation etc. impedes usability unless the domain is 3D (e.g. gaming, 3D modelling)
- Zooming is useful though
  - Overview of the data first
  - Zoom in to progressively add detail about what they are interested in and filter information they are not
  - Allows UI to provide details on demand.

Color:

- 8% males, 0.4% females have some form of color-deficiency:
- Types:
  - Protanomaly: red
  - Deuteranomaly: green
  - Tritanomaly: blue
- Least sensitive to blue

Reading:

- Saccades, fixations (94% of the time), regression
- Approx. 250 words/minute initially
- READING SPEED REDUCED BY ALL CAPS

#### Auditory

- Used dramatically less than vision
- About 20 Hz to 15-20 kHz
- Can adjust many parameters; amplitude, timbre, direction
- Filtering capabilities (e.g. cocktail party effect)
- Problems with signal interference and noise

#### Haptics

- Proprioception: sense of limb location, and
- Kinaesthesia: particularly limb movement, and
- Tactition: skin sensations

### Human Output

Motor response time depends on stimuli:

- Visual: ~200 ms
- Audio: ~150 ms
- Haptics: 700 ms
- Faster for combined signals

Muscle actions:

- Isotonic: contraction yields movement (e.g. mouse)
- Isometric: contraction with no movement (producing force but no movement e.g. ThinkPad TrackPoint™)

#### Fitts' Law

A very reliable model of rapid, aimed human movement.

- Predictive of tasks descriptive of devices
- Derived from Shannon's theory of capacity of information channels
  - *Signal*: amplitude $A$ of movement (or $D$ for distance) (middle of target)
  - *Noise*: width $W$ of target

*Index of difficulty (ID)* measures difficulty of rapid aimed movement:

$$
ID = log_2{\frac{A}{W} + 1}
$$

Measured in 'bits'.

Fitt's law: *movement time (MT)* is linear with ID:

$$
MT = a + b \cdot ID
   = a + b \cdot log_2{\frac{A}{W} + 1}
$$

$1/b$, the reciprocal of slope, is called *throughput*, or the *bandwidth* of the device in bits/second.

$a$ and $b$ are empirically determined. For a mouse:

- $a$ is typically 200-500 ms
- $b$ is typically 100-300 ms/bit

Typical velocity profile, validated for many types of aimed pointing:

```
Speed 
  ^
  |   Open-loop,
  |balistic impulse
  |    /\
  |   /  \   slow, closed-loop
  |  /    \    corrections
  | /      \  /\
  |/        \/  \/\___
  +------------------------>
           Time
```

#### Input Devices; Pointing & Scrolling

Human output is received as system input. There must be some sort of translation hardware to achieve this, which have many properties:

- Direct vs indirect
  - Touchscreens have perfect one-to-one correspondence
  - Trackpads indirect: mouse movement does not directly map to cursor movement
- Absolute vs relative
  - Touchscreens, pen-tablets
  - Trackpads (mostly) relative; finger location on the trackpad does not matter when moving the cursor
- Control
  - Position (zero-order) e.g. absolute pointing, dragging the scrollbar
  - Rate (first-order) e.g. holding down on mouse wheel and dragging up/down on Windows
  - Acceleration (second-order)
  - Note: having lots of modes and hence complexity may decrease number of interactions while making the task take longer due to the overhead of making decisions
- Isotonic: force with movement
- Isometric: force without movement e.g. 3D touch to control object size
- Control-display gain/transfer functions
  - Magic sauce of iOS inertial scrolling, Mac trackpads, etc.

The control-display transfer function:


- The input device (e.g. capacitive trackpad) sends device units
- The gain function scales the input in accordance with the user or environment settings
- Persistence is used to continue output even when there is no ongoing input, adding features such sa inertia

```
                                 Transfer Function
        +-------------------------------------------------------------------------+
        |                                                     e.g. scroll inertia |
        | device   ---------------  display   --------          ---------------   |
Device -+--------> | Translation | ---------> | Gain |  ------> | Persistence | --+---> Output
Input   | units    ---------------   units    --------          ---------------   |
        |                ^                       ^                     ^          |
        |                ------- Environment/User Settings ------------           |
        +-------------------------------------------------------------------------+
```

Scrolling transfer function for iOS:

- When the finger is on the screen there is direct mapping
- After the finger leaves, the speed slowly decays
- After four quick scroll gestures in quick succession, the scroll rate increases in an almost vertical line *after the finger leaves*
  - For all subsequent scroll gestures, the maximum velocity (immediately after the finger leaves) increases until a maximum scroll velocity is reached

#### Input Devices: Text Input

- Alternative keywords (e.g. Dvorak)
- Chord keys (e.g. stenographers)
- Constrained keyboards (e.g. T9 keyboards on old mobile phones)
- Reactive/predictive systems (autocomplete)
- Gestural input (e.g. swipe keyboards)
- Hand-writing recognition

Input expressibility: how well can you discriminate inputs? e.g. Google Glass had a tiny capacitive surface; doing text entry on that posed challenges.

#### Steering Law

Model of continuously controlled 'steering': moving an item across a given path, called a 'tunnel':

$MT = a + b \cdot ID = a + b \cdot \frac{A}{W}$

Where $A$ is the tunnel length and $W$ is the width. This can be used when $W$ varies.

This is important in cascading context menus, where hovering over an item overs a submenu to the left or right. Done naïvely, while travelling to the newly-opened submenu, the cursor must always stay above the item or the submenu will disappear. macOS appears to take into account the angle of travel to determine if the submenu should be hidden or not.

### Human Processing: Visual Search Time

If a person has to pick out a particular item out of $n$ items, the time $T$ taken to find the item increases linearly: $O(n)$.

This is slow, so the UI should aim to reduce the amount of searching the user must do. To achieve this, ensure there is **spatial stability**; items appear in the same place every time.
